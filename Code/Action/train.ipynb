{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np, torch, torch.nn as nn, torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ===============================\n",
    "# ÏÑ§Ï†ï\n",
    "# ===============================\n",
    "SAVE_PATH = \"/content/drive/MyDrive/ntu_best_model.pth\"\n",
    "ALLOWED_ACTIONS = ['A001','A002','A008','A009','A011','A012','A028','A029','A030','A041','A103','A104']\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ===============================\n",
    "# Dataset (12 ÎùºÎ≤® ÌïÑÌÑ∞ + Ïû¨Ïù∏Îç±Ïã± + velocity)\n",
    "# ===============================\n",
    "class NTUSkeletonDataset(data.Dataset):\n",
    "    def __init__(self, files, T=180):\n",
    "        self.files = files\n",
    "        self.T = T\n",
    "        self.allowed_actions = ALLOWED_ACTIONS\n",
    "        self.action2label = {a:i for i,a in enumerate(self.allowed_actions)}\n",
    "        # NTU25 bone pairs (1-based)\n",
    "        self.bone_pairs = [(1,2),(2,21),(3,21),(4,3),(5,21),(6,5),(7,6),(8,7),\n",
    "                        (9,21),(10,9),(11,10),(12,11),(13,1),(14,13),(15,14),\n",
    "                        (16,15),(17,1),(18,17),(19,18),(20,19),(22,23),(23,8),\n",
    "                        (24,25),(25,12)]\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def _pad(self, arr):\n",
    "        T,V,C = arr.shape\n",
    "        out = np.zeros((self.T,V,C),dtype=np.float32)\n",
    "        if T >= self.T:\n",
    "            # Ï§ëÏïô ÌÅ¨Î°≠(Í∏∏Ïù¥ Îã§ÏñëÏÑ±Ïóê Í∞ïÌï®)\n",
    "            s = (T - self.T)//2\n",
    "            out[:] = arr[s:s+self.T]\n",
    "        else:\n",
    "            out[:T] = arr\n",
    "        return out\n",
    "\n",
    "    def _normalize(self, J):\n",
    "        # Ìûô Ï§ëÏã¨(0Î≤à) Ïù¥Îèô\n",
    "        J = J - J[:,0:1,:]\n",
    "        # Ïä§ÏºÄÏùº: Ïñ¥Íπ®Ìè≠(5,9) Ïö∞ÏÑ†, Ïã§Ìå®Ïãú Í≥®Î∞òÌè≠(13,17)\n",
    "        shoulder = np.linalg.norm(J[:,4:5,:] - J[:,8:9,:], axis=-1) # 0-based\n",
    "        pelvis   = np.linalg.norm(J[:,12:13,:] - J[:,16:17,:], axis=-1)\n",
    "        denom = np.where(np.isfinite(shoulder) & (shoulder>1e-6), shoulder, pelvis)\n",
    "        s = denom.mean()\n",
    "        if not np.isfinite(s) or s < 1e-6: s = 1.0\n",
    "        return J / s\n",
    "\n",
    "    def _bone(self, J):\n",
    "        B = np.zeros_like(J)\n",
    "        for a,b in self.bone_pairs:\n",
    "            B[:, b-1] = J[:, b-1] - J[:, a-1]\n",
    "        return B\n",
    "\n",
    "    def _vel(self, J):\n",
    "        Vv = np.zeros_like(J)\n",
    "        Vv[1:] = J[1:] - J[:-1]\n",
    "        return Vv\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        d = np.load(path, allow_pickle=True).item()\n",
    "        J = d['skel_body0'].astype('float32')      # (T,V,3)\n",
    "        J = self._pad(J)\n",
    "        J = self._normalize(J)\n",
    "        B = self._bone(J)\n",
    "        Vv= self._vel(J)\n",
    "\n",
    "        # (C,T,V)\n",
    "        J  = np.transpose(J , (2,0,1))\n",
    "        B  = np.transpose(B , (2,0,1))\n",
    "        Vv = np.transpose(Vv, (2,0,1))\n",
    "\n",
    "        base = os.path.basename(path)\n",
    "        Acode = None\n",
    "        for a in self.allowed_actions:\n",
    "            if a in base: Acode = a; break\n",
    "        y = self.action2label[Acode]\n",
    "\n",
    "        return torch.tensor(J), torch.tensor(B), torch.tensor(Vv), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ===============================\n",
    "# Split (ÎûúÎç§ 8:1:1) + Cross-Subject ÏòµÏÖò\n",
    "# ===============================\n",
    "def list_allowed_files(root):\n",
    "    all_files = [os.path.join(root,f) for f in os.listdir(root) if f.endswith('.npy')]\n",
    "    files = [f for f in all_files if any(a in f for a in ALLOWED_ACTIONS)]\n",
    "    print(f\"‚úÖ ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ Í∞úÏàò: {len(files)} / {len(all_files)}\")\n",
    "    return files\n",
    "\n",
    "def parse_ids(fname):\n",
    "    # S001C002P003R001A004.npy\n",
    "    base = os.path.basename(fname)\n",
    "    S = int(base.split('S')[1].split('C')[0])\n",
    "    C = int(base.split('C')[1].split('P')[0])\n",
    "    P = int(base.split('P')[1].split('R')[0])\n",
    "    return S,C,P\n",
    "\n",
    "def split_random(files, train=0.8, val=0.1, test=0.1, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(files)); rng.shuffle(idx)\n",
    "    n=len(files); n_tr=int(n*train); n_val=int(n*val)\n",
    "    tr = [files[i] for i in idx[:n_tr]]\n",
    "    va = [files[i] for i in idx[n_tr:n_tr+n_val]]\n",
    "    te = [files[i] for i in idx[n_tr+n_val:]]\n",
    "    print(f\"TRAIN: {len(tr)} | VAL: {len(va)} | TEST: {len(te)}\")\n",
    "    return tr,va,te\n",
    "\n",
    "def split_cross_subject_811(files, seed=SEED):\n",
    "    \"\"\"\n",
    "    ÏÇ¨Îûå(Subject) Í∏∞Ï§ÄÏúºÎ°ú 8:1:1 ÎπÑÏú®Î°ú Train/Val/Test Î∂ÑÌï†\n",
    "    \"\"\"\n",
    "    # 1Ô∏è‚É£ ÏÇ¨Îûå ID (P###) Ï∂îÏ∂ú\n",
    "    subs = sorted({parse_ids(f)[2] for f in files})\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(subs)\n",
    "\n",
    "    # 2Ô∏è‚É£ ÏÇ¨Îûå Îã®ÏúÑÎ°ú Î∂ÑÌï† (8:1:1)\n",
    "    n = len(subs)\n",
    "    n_train = int(n * 0.8)\n",
    "    n_val   = int(n * 0.1)\n",
    "    train_subs = set(subs[:n_train])\n",
    "    val_subs   = set(subs[n_train:n_train+n_val])\n",
    "    test_subs  = set(subs[n_train+n_val:])\n",
    "\n",
    "    # 3Ô∏è‚É£ ÌååÏùºÏùÑ ÏÇ¨Îûå Îã®ÏúÑÎ°ú Î∂ÑÎ∞∞\n",
    "    tr = [f for f in files if parse_ids(f)[2] in train_subs]\n",
    "    va = [f for f in files if parse_ids(f)[2] in val_subs]\n",
    "    te = [f for f in files if parse_ids(f)[2] in test_subs]\n",
    "\n",
    "    print(f\"[Cross-Subject 8:1:1] TRAIN: {len(tr)} | VAL: {len(va)} | TEST: {len(te)}\")\n",
    "    return tr, va, te\n",
    "\n",
    "# ===============================\n",
    "# Î™®Îç∏: Í≤ΩÎüâ CTR-GCN Ïä§ÌÉÄÏùº + Dilated TCN + GroupNorm\n",
    "# ===============================\n",
    "class JointMix(nn.Module):\n",
    "    \"\"\"Learnable joint mixing across V dimension (VxV), CTR-GCNÏùò Ï±ÑÎÑê-Í≥µÍ∞Ñ ÌòºÌï© ÏïÑÏù¥ÎîîÏñ¥ Í≤ΩÎüâÌôî.\"\"\"\n",
    "    def __init__(self, V):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.eye(V))  # (V,V)\n",
    "    def forward(self, x):\n",
    "        # x: (N,C,T,V)\n",
    "        return torch.einsum('nctv,vw->nctw', x, self.W)\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, dilation=1, groups=8, drop=0.1):\n",
    "        super().__init__()\n",
    "        pad = (k-1)//2 * dilation\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=(k,1), padding=(pad,0), dilation=(dilation,1))\n",
    "        self.gn   = nn.GroupNorm(groups, out_ch)\n",
    "        self.act  = nn.SiLU(inplace=True)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.res  = (in_ch==out_ch)\n",
    "        self.proj = nn.Identity() if self.res else nn.Conv2d(in_ch,out_ch,1)\n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        y = self.gn(y); y = self.act(y); y = self.drop(y)\n",
    "        return self.proj(x) + y\n",
    "\n",
    "class StreamNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, V=25, num_classes=12, width=(64,128,256)):\n",
    "        super().__init__()\n",
    "        C1,C2,C3 = width\n",
    "        self.stem = nn.Conv2d(in_ch, C1, 1)\n",
    "        self.mix1 = JointMix(V)\n",
    "        self.t1a  = TBlock(C1, C1, dilation=1)\n",
    "        self.t1b  = TBlock(C1, C1, dilation=2)\n",
    "        self.mix2 = JointMix(V)\n",
    "        self.t2a  = TBlock(C1, C2, dilation=2)\n",
    "        self.t2b  = TBlock(C2, C2, dilation=4)\n",
    "        self.mix3 = JointMix(V)\n",
    "        self.t3a  = TBlock(C2, C3, dilation=2)\n",
    "        self.t3b  = TBlock(C3, C3, dilation=4)\n",
    "        self.head = nn.Linear(C3, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: (N,C,T,V)\n",
    "        x = self.stem(x)\n",
    "        x = self.mix1(x); x = self.t1a(x); x = self.t1b(x)\n",
    "        x = self.mix2(x); x = self.t2a(x); x = self.t2b(x)\n",
    "        x = self.mix3(x); x = self.t3a(x); x = self.t3b(x)\n",
    "        x = x.mean(dim=[2,3])  # GAP\n",
    "        return self.head(x)\n",
    "\n",
    "class ThreeStream(nn.Module):\n",
    "    def __init__(self, num_classes=12, V=25):\n",
    "        super().__init__()\n",
    "        self.j = StreamNet(in_ch=3, V=25, num_classes=num_classes)\n",
    "        self.b = StreamNet(in_ch=3, V=25, num_classes=num_classes)\n",
    "        self.v = StreamNet(in_ch=3, V=25, num_classes=num_classes)\n",
    "        self.fc = nn.Linear(num_classes*3, num_classes)\n",
    "    def forward(self, J,B,Vv):\n",
    "        pj = self.j(J); pb = self.b(B); pv = self.v(Vv)\n",
    "        return self.fc(torch.cat([pj,pb,pv], dim=-1))\n",
    "\n",
    "# ===============================\n",
    "# ÌèâÍ∞Ä/ÌïôÏäµ Î£®ÌîÑ\n",
    "# ===============================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device, desc=\"VAL\"):\n",
    "    model.eval()\n",
    "    tl, correct, n = 0.0, 0, 0\n",
    "    for J,B,Vv,y in loader:\n",
    "        J,B,Vv,y = J.to(device), B.to(device), Vv.to(device), y.to(device)\n",
    "        with autocast():\n",
    "            logits = model(J,B,Vv)\n",
    "            loss = loss_fn(logits, y)\n",
    "        tl += loss.item()\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        n += y.size(0)\n",
    "    avg = tl / max(1,len(loader))\n",
    "    acc = correct / max(1,n)\n",
    "    print(f\"üìè {desc}  loss={avg:.4f}  acc={acc:.2%}\")\n",
    "    return avg, acc\n",
    "\n",
    "def train(\n",
    "    data_folder=\"/content/drive/MyDrive/ntu_rgbd_npy_backup/raw_npy\",\n",
    "    epochs=25, batch_size=4, accum_steps=4, T=180, use_cross_subject=False\n",
    "):\n",
    "    # Drive mount\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    pin = torch.cuda.is_available()\n",
    "\n",
    "    files = list_allowed_files(data_folder)\n",
    "    if use_cross_subject:\n",
    "        tr_files, va_files, te_files = split_cross_subject_811(files)\n",
    "    else:\n",
    "        tr_files, va_files, te_files = split_random(files)\n",
    "\n",
    "    train_set = NTUSkeletonDataset(tr_files, T=T)\n",
    "    val_set   = NTUSkeletonDataset(va_files, T=T)\n",
    "    test_set  = NTUSkeletonDataset(te_files, T=T)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin)\n",
    "    val_loader   = data.DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "    test_loader  = data.DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "\n",
    "    num_classes = len(ALLOWED_ACTIONS)\n",
    "    model = ThreeStream(num_classes=num_classes).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # warmup + cosine\n",
    "    def lr_lambda(cur_step):\n",
    "        warm = max(1, int(0.1*epochs))*len(train_loader)\n",
    "        if cur_step < warm:\n",
    "            return (cur_step+1)/warm\n",
    "        # cosine\n",
    "        t = (cur_step - warm) / max(1,(epochs*len(train_loader)-warm))\n",
    "        return 0.5*(1+np.cos(np.pi*t))\n",
    "    sched = optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    global_step = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        for i,(J,B,Vv,y) in enumerate(pbar):\n",
    "            J,B,Vv,y = J.to(device, non_blocking=True), B.to(device, non_blocking=True), Vv.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            with autocast():\n",
    "                logits = model(J,B,Vv)\n",
    "                loss = loss_fn(logits, y) / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i+1) % accum_steps == 0:\n",
    "                scaler.unscale_(opt)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "            running += loss.item()*accum_steps\n",
    "            pbar.set_postfix(loss=f\"{(loss.item()*accum_steps):.4f}\")\n",
    "            global_step += 1\n",
    "            sched.step()\n",
    "\n",
    "        train_loss = running / max(1,len(train_loader))\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, loss_fn, device, \"VAL\")\n",
    "        print(f\"Epoch {epoch} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.2%}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            print(f\"üíæ Best updated ‚Üí {SAVE_PATH} (val_loss={best_val:.4f})\")\n",
    "\n",
    "    # ÏµúÏ¢Ö ÌÖåÏä§Ìä∏\n",
    "    model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
    "    test_loss, test_acc = evaluate(model, test_loader, loss_fn, device, \"TEST\")\n",
    "    print(f\"‚úÖ Test done | loss={test_loss:.4f} acc={test_acc:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(\n",
    "        data_folder=\"/content/drive/MyDrive/ntu_rgbd_npy_backup/raw_npy\",\n",
    "        epochs=25, batch_size=4, accum_steps=4, T=180, use_cross_subject=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
